# Project Overview

Have you ever wanted to get into a podcast episode, but didn’t listen because it was too long? We are working on a podcast summarization tool called TL;DL (Too Long; Didn't Listen), which transcribes a podcast's content and provides an easy to read summarization - no ads, no promotions. Reading a condensed summary of details from a podcast episode can give you a sense of what a new show focuses on or help you catch up on hours’ worth of content in minutes. Whether you are catching up on previous episodes or browsing for something new, TL;DL will give you the synopses of the podcast of your choice and help you make better use of your time.

Unlike radio content, which directly broadcasts content over the airwaves, podcasts now capture, organize and make content available for listening at a time of our choosing.  Given the all but endless supply of audio content now available for individuals to explore, many uses exist for a summarization, which extends beyond just a hook or an overview of the content.  Such opportunities arise in the informational, educational, and pure entertainment arenas.

### TL;DL Utility

Podcasts and its supporting listenership have exploded in recent years.  The podcast industry is targeted to reach $ 1 billion in 2021. ([BusinessInsider](https://www.businessinsider.com/the-podcast-industry-report)) A 16% increase in listeners is expected in 2021, with the total number of listeners having doubled since 2016.  Principle US market share holders include Apple, Spotify, YouTube and Google, with the first two possessing the majority share of the market.

Topics covered by podcasts are as diverse as listeners.  Though content is more readily available than ever before, what has not changed is the time needed to consume it.  The average length of podcasts is around 30 minutes; however, this distribution is skewed to the right.  (include link to spotify metrics)  A functional gap exists to encapsulate content from the surfeit of available podcasts.  Further, existing synopses tend to be inconsistent and sporadic at best.

Beyond the immediate summarization of content for podcast listeners, TL;DL can offer substantial support to entities that curate audio content.  Whether it be public libraries or private companies, TL;DL can support numerous requirements within the audio industry.

### Sample Summarizations

To give a sample of what TL;DL can provide, below are two summaries generated by this tool.

[Broadcast Sample 1]()

[Broadcast Sample 2]()

### Try Your Own Transcript

Use TL;DL to summarize an audio file of your choosing!

[Go to Model]()

# Listeners & Their Needs

To capture insights from current podcast listeners, a survey was distributed to a variety of sights, to include LinkedIn, Facebook Groups, and the MIDS Slack channel.  The survey results confirmed the the validity of initial assumptions and the potential interest in a text summarization tool for audio files.

### Types of Listeners

Three distinct audience groups were identified within the podcast listenership.  These were:

  * Stalwart listeners searching with time constraints
  * Enthusiasts looking to quickly catch up on missed episodes
  * New listeners looking for targeted broadcasts

On a weekly basis, respondents had the following listening trends:

  * Followed on average 3 Podcasts
  * More than 50% listened to at least 1 hour of Podcasts
  * More than 50% listened to broadcasts at least 30 minutes long

### Concept Utility

A founding assumption for this project was that listeners did not finish audio broadcasts.  Given that, audio synopses would actually be desired amongst listeners.  Survey results did confirm that many listeners did not finish broadcasts; however, granularity on rationale for stopping was not collected.  Respondents did consistently convey value for text summarizations.  Overall, 80% perceived that some level of value existed, with 40% indicating a moderate to high value.  A variety of utiliziations were provided in the survey.  Grouping responses into primary and secondary catagorizations created the following results:

**Primary Utilizations (~60%)**
  * Obtaining talking points & takaways from a broadcasts
  * Serving as a decision tool when exploring new Podcasts

**Secondary Utilizations (~40%)**
  * Catching up on missed episodes
  * Revisiting boradcasts previously listened to

## Value Proposition

Independent research and survey results coalesed into an understanding of the value proposition that TL;DL would provide to listeners and audio consumers in general.

**Task:**  To support the efficient consumption of audio consistent

**Pains:**
  * Burdensome time commitment
  * Uncertainity on the level of interest with any one single audio files

**Gains:**
  * Enhanced listener experience
  * Expanded Listenership
  * Easy to read summarizations for current or archived use
  
  ![value_prop2](https://user-images.githubusercontent.com/59575598/126886301-1ac2c5e8-bec6-4a45-81de-721762049653.png)

# Methodology

The supporting pipeline for this project is both simple and straight forward.  Upon identifying a podcast audio file, the text is transcribed and then sent to the summarization model.  The returned abstractive summary is an easily digestable text that is approximately a paragraph in length.

![pipeline](https://user-images.githubusercontent.com/59575598/126886310-7a6b5fd2-9e25-4b66-89a3-f72eb2dfc04a.png)

## Transcription

Google Cloud's Speech to Text API handles the transcription portion of the pipeline.  This out-of-the-box solution has many advantages, though it does impose some limitations on the final output.

### Advantages

Google Cloud's Speech to Text API provides access to extensively developed speech recognition models.  Further, these robust models performs well with multiple speakers.  Since many podcasts have multiple speakers, this feature is essential to this application.  Finally, with an eye to the future, this solution is also highly scalable, as the computational requirements of processing audio data are offloaded to the cloud, which simplifies management of computational resources.

### Disadvantages

Cost represents the biggest disadvantage with this transcription approach.  For the enhanced model capable of processing audio from multiple speakers, and data logging opt-in enabled to reduce costs, audio transcription costs 0.6 cents per 15 second increment.  This translates to 2.4 cents per minute, and $1.44 for a 60-minute podcast episode.  Individually, this is not a concern; however, it would become an issue at scale.  It also places a key element of the pipeline's functionality on third-party resources.  With that, it necessitates remaining vigilant about changes to the API and it provider.

## Summarization Model

Initial summarization efforts focused on extractive synopses.  These failed to meet the intent of TL;DL.  This approach tended to latch onto advertisements within podcasts, or other frequently repeated phrasing at the expense of capturing the overall content of the podcast itself.  Though more elusive in nature, an abstrative model proved to be the appropriate solution for the TL;DL application.  Numerous summarization models were explored, with varying levels of results.  The model that generated the best and most consistent results was PEGASUS, Pre-training with Extracted Gap-sentences for Abstrative Summarization.

### PEGASUS

PEGASUS is a state-of-the-art model for abstractive text summarization developed by Google researched and published at the 2020 International Conference on Machine Learning.  See links provided in additional readings for further information on the PEGASUS model from its designers.

After experimenting on the 12 datasets used to fine-tune PEGASUS, clear winners emerged for short and long text summaries.  For the short summary, fine-tining on the [xSum](https://www.tensorflow.org/datasets/catalog/xsum) dataset generated the most concise one sentence summarizations.  This dataset is sourced from 227,000 diverse BBC news articles from 2010 to 2017.  Each of the articles is accompanied by a professionally written single-sentence summary for training.  For the long summary, the best results emerged when fine-tuned on [Multi-news](https://github.com/Alex-Fabbri/Multi-News), a 56,000 multip-news corpus of news articles. This data set was accompanied by human-written summaries for the artiles from the newser.com site.  Results from the other data sets produced either incomprensible or repetitive summaries, common problems when attempting to summarize text. Below is a sample of the results obtained when using the PEGASUS model.

![transcript_sample](https://user-images.githubusercontent.com/59575598/126886366-6c368b75-1804-470e-99b7-327130332db9.png)

### TL;DL Application

The biggest challenge with using the PEGASUS model is the fact that all the results are zero-shot summarizations.  PEGASUS was trained to summarize news articles, but our task is to summarize podcast transcripts.  The distinct differences in these types  of transcripts necessitated that a corpus of manually summarized podcast transcripts serve as the training base, rather than the original new article ones.

# Way Forward

### Met Challenges

Much was learned and accomplished in the development of the TL;DL model.  A unexpected challenge faced was generation of false information in the abstractive summaries.  Though visually satisfying and coherently written, they did not consistently reflect the actual content in their associated transcript file.

### Future Work

Several areas are rife with potential future development.

* **Refine transcription process.**  Using speech-to-text APIs has the potential to become cost prohibitive.  Developing solutions to mitigate this and or create libraries of summarizations to avoid redundant costs would prove beneficial to listeners.

* **Improve Summarization Model.**

### Additonal Readings

[PEGASUS_Paper](https://arxiv.org/abs/1912.08777)

[PEGASUS_blog](https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html)

[Fictional_Abstractive_Summaries](https://towardsdatascience.com/entity-level-factual-consistency-in-abstractive-text-summarization-cb19e8a48397)

# Team Members

Inspiration for this project came from avid podcast listeners who were also practicing data scientists.  Brought together by the [UC Berkeley MIDS program](https://ischoolonline.berkeley.edu/data-science/), this project represents a capstone experience that not only supported degree requirements, but also honed skills necessary to tackle present and future data challenges.

Kevin Pong: kpong@ischool.berkeley.edu

Cecily Sun: cecily.sun@ischool.berkeley.edu

Patricia Dominguez:  pdoming@ischool.berkeley.edu

Jill Cheney:  jill.n.cheney@ischool.berkely.edu
